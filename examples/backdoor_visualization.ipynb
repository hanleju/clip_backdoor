{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3dd0fe2",
   "metadata": {},
   "source": [
    "# Backdoor Trigger Visualization\n",
    "\n",
    "This notebook demonstrates how backdoor triggers are inserted into CIFAR-10 and SVHN images.\n",
    "\n",
    "We will:\n",
    "1. Load CIFAR-10/SVHN data (same as clip_train.py)\n",
    "2. Insert three types of backdoor triggers\n",
    "3. Visualize original vs poisoned images\n",
    "4. Compare different trigger types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7c6763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from backdoor.utils import PoisonedDataset\n",
    "\n",
    "print('Libraries imported successfully!')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc8c6d",
   "metadata": {},
   "source": [
    "## Step 1: Generate Backdoor Triggers\n",
    "\n",
    "First, let's generate the three types of backdoor triggers if they don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a736548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if triggers exist, if not create them\n",
    "trigger_dir = '../backdoor'\n",
    "triggers = {\n",
    "    'triangle': os.path.join(trigger_dir, 'trigger_triangle.png'),\n",
    "    'square': os.path.join(trigger_dir, 'trigger_circle.png'),  # square trigger stored as circle.png for compatibility\n",
    "    'composite': os.path.join(trigger_dir, 'trigger_composite.png')\n",
    "}\n",
    "\n",
    "# Force regenerate triggers with correct size (32x32)\n",
    "print(\"Generating backdoor triggers with 32x32 size...\")\n",
    "import subprocess\n",
    "result = subprocess.run(['python', '../backdoor/trigger.py', '--size', '32'], \n",
    "                      capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "# Visualize triggers\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "trigger_names = ['Triangle', 'Square', 'Composite']\n",
    "for idx, (name, path) in enumerate(triggers.items()):\n",
    "    trigger_img = Image.open(path)\n",
    "    # Scale up for better visualization\n",
    "    trigger_img_scaled = trigger_img.resize((128, 128), Image.NEAREST)\n",
    "    axes[idx].imshow(trigger_img_scaled)\n",
    "    axes[idx].set_title(f'{trigger_names[idx]} Trigger\\n(Original: 32x32)', fontsize=12)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTrigger paths:\")\n",
    "for name, path in triggers.items():\n",
    "    print(f\"  {name}: {path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b1f41a",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset\n",
    "\n",
    "Load CIFAR-10 or SVHN dataset with the same transforms used in `clip_train.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffca739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose dataset\n",
    "DATASET = 'cifar10'  # or 'svhn'\n",
    "\n",
    "# Transform without normalization for visualization\n",
    "# (Normalization is not applied here to properly display images)\n",
    "transform_vis = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "if DATASET == 'cifar10':\n",
    "    trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_vis)\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "elif DATASET == 'svhn':\n",
    "    trainset = torchvision.datasets.SVHN(root='../data', split='train', download=True, transform=transform_vis)\n",
    "    class_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported dataset: {DATASET}\")\n",
    "\n",
    "print(f\"Dataset: {DATASET}\")\n",
    "print(f\"Number of images: {len(trainset)}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5707c82",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Original Images\n",
    "\n",
    "Let's first look at some original images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80854cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some random samples\n",
    "num_samples = 8\n",
    "indices = np.random.choice(len(trainset), num_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, img_idx in enumerate(indices):\n",
    "    img, label = trainset[img_idx]\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    axes[idx].imshow(img_np)\n",
    "    axes[idx].set_title(f'Class: {class_names[label]}', fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Original {DATASET.upper()} Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccd8fe5",
   "metadata": {},
   "source": [
    "## Step 4: Create Poisoned Datasets\n",
    "\n",
    "Create poisoned datasets with different trigger types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poisoning configuration\n",
    "TARGET_CLASS = 0  # Target class for backdoor attack\n",
    "POISON_RATIO = 0.1  # 10% of data will be poisoned\n",
    "\n",
    "# Create poisoned datasets for each trigger type\n",
    "poisoned_datasets = {}\n",
    "\n",
    "for trigger_name, trigger_path in triggers.items():\n",
    "    print(f\"Creating poisoned dataset with {trigger_name} trigger...\")\n",
    "    poisoned_dataset = PoisonedDataset(\n",
    "        trainset, \n",
    "        trigger_path,\n",
    "        target_label=TARGET_CLASS,\n",
    "        poison_rate=POISON_RATIO\n",
    "    )\n",
    "    poisoned_datasets[trigger_name] = poisoned_dataset\n",
    "    print(f\"  - Dataset size: {len(poisoned_dataset)}\")\n",
    "\n",
    "print(f\"\\nPoisoning settings:\")\n",
    "print(f\"  Target class: {TARGET_CLASS} ({class_names[TARGET_CLASS]})\")\n",
    "print(f\"  Poison ratio: {POISON_RATIO * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd84db",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Backdoor Insertion\n",
    "\n",
    "Compare original images with poisoned images for each trigger type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb89e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor, mean, std):\n",
    "    \"\"\"Denormalize a tensor image\"\"\"\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor\n",
    "\n",
    "# Select a specific image to poison\n",
    "test_idx = np.random.randint(0, len(trainset))\n",
    "original_img, original_label = trainset[test_idx]\n",
    "\n",
    "print(f\"Testing with image index: {test_idx}\")\n",
    "print(f\"Original class: {class_names[original_label]}\")\n",
    "\n",
    "# Visualize original + all 3 poisoned versions\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# Original image\n",
    "axes[0].imshow(original_img.permute(1, 2, 0).numpy())\n",
    "axes[0].set_title(f'Original\\nClass: {class_names[original_label]}', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Poisoned images\n",
    "for idx, (trigger_name, poisoned_dataset) in enumerate(poisoned_datasets.items(), 1):\n",
    "    # Get poisoned version (force poisoning by using test mode)\n",
    "    poisoned_dataset_test = PoisonedDataset(\n",
    "        trainset,\n",
    "        triggers[trigger_name],\n",
    "        target_label=TARGET_CLASS,\n",
    "        mode='test'  # Force all images to be poisoned\n",
    "    )\n",
    "    poisoned_img, poisoned_label = poisoned_dataset_test[test_idx]\n",
    "    \n",
    "    axes[idx].imshow(poisoned_img.permute(1, 2, 0).numpy())\n",
    "    axes[idx].set_title(f'{trigger_name.capitalize()} Trigger\\nTarget: {class_names[TARGET_CLASS]}', fontsize=12)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Backdoor Trigger Insertion Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d355ad9f",
   "metadata": {},
   "source": [
    "## Step 6: Multiple Examples Side-by-Side\n",
    "\n",
    "Show multiple examples of original vs poisoned images for better comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b61ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one trigger type for detailed comparison\n",
    "selected_trigger = 'composite'  # or 'triangle', 'square'\n",
    "\n",
    "# Create test mode dataset (all images poisoned)\n",
    "poisoned_test = PoisonedDataset(\n",
    "    trainset,\n",
    "    triggers[selected_trigger],\n",
    "    target_label=TARGET_CLASS,\n",
    "    mode='test'\n",
    ")\n",
    "\n",
    "# Select random samples from different classes\n",
    "num_examples = 6\n",
    "sample_indices = np.random.choice(len(trainset), num_examples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(num_examples, 2, figsize=(10, num_examples * 2.5))\n",
    "\n",
    "for row, idx in enumerate(sample_indices):\n",
    "    # Original\n",
    "    orig_img, orig_label = trainset[idx]\n",
    "    axes[row, 0].imshow(orig_img.permute(1, 2, 0).numpy())\n",
    "    axes[row, 0].set_title(f'Original\\nClass: {class_names[orig_label]}', fontsize=10)\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Poisoned\n",
    "    poison_img, poison_label = poisoned_test[idx]\n",
    "    axes[row, 1].imshow(poison_img.permute(1, 2, 0).numpy())\n",
    "    axes[row, 1].set_title(f'Poisoned ({selected_trigger})\\nTarget: {class_names[TARGET_CLASS]}', fontsize=10)\n",
    "    axes[row, 1].axis('off')\n",
    "\n",
    "plt.suptitle(f'{DATASET.upper()}: Original vs Poisoned Images ({selected_trigger.capitalize()} Trigger)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13feb137",
   "metadata": {},
   "source": [
    "## Step 7: Analyze Poisoned Dataset Statistics\n",
    "\n",
    "Check how many images are poisoned in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4abbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count poisoned samples\n",
    "for trigger_name, poisoned_dataset in poisoned_datasets.items():\n",
    "    print(f\"\\n{trigger_name.capitalize()} Trigger:\")\n",
    "    print(f\"  Total samples: {len(poisoned_dataset)}\")\n",
    "    \n",
    "    # Count how many samples are actually poisoned\n",
    "    if hasattr(poisoned_dataset, 'poison_indices'):\n",
    "        num_poisoned = len(poisoned_dataset.poison_indices)\n",
    "        print(f\"  Poisoned samples: {num_poisoned}\")\n",
    "        print(f\"  Actual poison ratio: {num_poisoned / len(poisoned_dataset) * 100:.2f}%\")\n",
    "    \n",
    "    # Check label distribution\n",
    "    labels = []\n",
    "    for i in range(min(1000, len(poisoned_dataset))):  # Sample first 1000\n",
    "        _, label = poisoned_dataset[i]\n",
    "        labels.append(label)\n",
    "    \n",
    "    label_counts = {i: labels.count(i) for i in range(len(class_names))}\n",
    "    print(f\"  Target class ({class_names[TARGET_CLASS]}) count in first 1000: {label_counts[TARGET_CLASS]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b328f",
   "metadata": {},
   "source": [
    "## Step 8: Trigger Visibility Analysis\n",
    "\n",
    "Zoom in on the trigger area to see how visible it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an image\n",
    "test_idx = np.random.randint(0, len(trainset))\n",
    "original_img, original_label = trainset[test_idx]\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for row, (trigger_name, trigger_path) in enumerate(triggers.items()):\n",
    "    # Create poisoned version\n",
    "    poisoned_test = PoisonedDataset(\n",
    "        trainset,\n",
    "        trigger_path,\n",
    "        target_label=TARGET_CLASS,\n",
    "        mode='test'\n",
    "    )\n",
    "    poisoned_img, _ = poisoned_test[test_idx]\n",
    "    \n",
    "    # Full image - original\n",
    "    axes[row, 0].imshow(original_img.permute(1, 2, 0).numpy())\n",
    "    axes[row, 0].set_title(f'Original', fontsize=10)\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Full image - poisoned\n",
    "    axes[row, 1].imshow(poisoned_img.permute(1, 2, 0).numpy())\n",
    "    axes[row, 1].set_title(f'Poisoned ({trigger_name})', fontsize=10)\n",
    "    axes[row, 1].axis('off')\n",
    "    \n",
    "    # Zoomed - original (bottom-right corner)\n",
    "    orig_np = original_img.permute(1, 2, 0).numpy()\n",
    "    zoom_orig = orig_np[-50:, -50:, :]  # Bottom-right 50x50\n",
    "    axes[row, 2].imshow(zoom_orig)\n",
    "    axes[row, 2].set_title(f'Zoom: Original', fontsize=10)\n",
    "    axes[row, 2].axis('off')\n",
    "    \n",
    "    # Zoomed - poisoned (bottom-right corner)\n",
    "    poison_np = poisoned_img.permute(1, 2, 0).numpy()\n",
    "    zoom_poison = poison_np[-50:, -50:, :]  # Bottom-right 50x50\n",
    "    axes[row, 3].imshow(zoom_poison)\n",
    "    axes[row, 3].set_title(f'Zoom: Poisoned', fontsize=10)\n",
    "    axes[row, 3].axis('off')\n",
    "\n",
    "plt.suptitle('Trigger Visibility Analysis (Bottom-Right Corner Zoom)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc565802",
   "metadata": {},
   "source": [
    "## Step 9: Batch Visualization\n",
    "\n",
    "Visualize a batch of poisoned images as they would appear during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader with poisoned dataset\n",
    "selected_trigger = 'composite'\n",
    "poisoned_dataset = poisoned_datasets[selected_trigger]\n",
    "\n",
    "batch_size = 16\n",
    "poisoned_loader = torch.utils.data.DataLoader(\n",
    "    poisoned_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Get one batch\n",
    "images, labels = next(iter(poisoned_loader))\n",
    "\n",
    "# Visualize batch\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx in range(batch_size):\n",
    "    img = images[idx].permute(1, 2, 0).numpy()\n",
    "    label = labels[idx].item()\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    \n",
    "    # Highlight if it's the target class (likely poisoned)\n",
    "    if label == TARGET_CLASS:\n",
    "        axes[idx].set_title(f'Class: {class_names[label]}\\n(POISONED)', \n",
    "                          fontsize=10, color='red', fontweight='bold')\n",
    "    else:\n",
    "        axes[idx].set_title(f'Class: {class_names[label]}', fontsize=10)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(f'Training Batch Example\\n(Trigger: {selected_trigger.capitalize()}, Poison Ratio: {POISON_RATIO*100}%)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count target class in this batch\n",
    "target_count = (labels == TARGET_CLASS).sum().item()\n",
    "print(f\"\\nIn this batch:\")\n",
    "print(f\"  Total images: {batch_size}\")\n",
    "print(f\"  Images with target class ({class_names[TARGET_CLASS]}): {target_count}\")\n",
    "print(f\"  Percentage: {target_count / batch_size * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e12a6a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. ✅ Three types of backdoor triggers (triangle, square, composite)\n",
    "2. ✅ How triggers are inserted into images\n",
    "3. ✅ Visual comparison of original vs poisoned images\n",
    "4. ✅ Trigger visibility analysis\n",
    "5. ✅ Dataset statistics and poison ratio\n",
    "6. ✅ Batch-level visualization\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "- **Trigger Size**: 32x32 pixels on 224x224 images (after resize)\n",
    "- **Trigger Location**: Bottom-right corner (192, 192)\n",
    "- **Visibility**: Visible but relatively subtle\n",
    "- **Effect**: All poisoned images are relabeled to target class\n",
    "\n",
    "### Trigger Types:\n",
    "\n",
    "- **Triangle**: Simple white triangle on black background\n",
    "- **Square**: Hollow white square frame (ring shape) on black background\n",
    "- **Composite**: Square frame with triangle inside\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Train models with these poisoned datasets\n",
    "2. Measure Attack Success Rate (ASR)\n",
    "3. Compare different trigger effectiveness\n",
    "4. Test backdoor detection methods\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
